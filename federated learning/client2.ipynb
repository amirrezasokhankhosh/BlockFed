{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import req\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client():\n",
    "    def __init__(self, name, X, y, learning_rate):\n",
    "        self.name = name\n",
    "        self.X = tf.constant(X)\n",
    "        self.y = tf.constant(y)\n",
    "        self.W = tf.Variable(np.zeros((2, 1)))\n",
    "        self.b = tf.Variable(0, dtype=tf.double)\n",
    "        self.round = 0\n",
    "        self.epochs = 100\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def check_trian(self):\n",
    "        curr_r = req.get_round()\n",
    "        if curr_r > self.round:\n",
    "            self.round = curr_r\n",
    "            self.update_local_model()\n",
    "            self.train()\n",
    "            self.send_local_model()\n",
    "            print(\"Trained global model.\")\n",
    "        else:\n",
    "            print(\"Round is stil going on.\")\n",
    "    \n",
    "    def update_local_model(self):\n",
    "        global_p = req.get_pre_global()\n",
    "        W, b = global_p['W'], global_p['b']\n",
    "        self.W = tf.Variable(np.array(W).reshape((2, 1)))\n",
    "        self.b = tf.Variable(b, dtype=tf.double)\n",
    "\n",
    "    def train(self):\n",
    "        for _ in range(self.epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch([self.W, self.b])\n",
    "                phi = self.X @ self.W + self.b\n",
    "                pred = tf.keras.activations.sigmoid(phi)\n",
    "                loss = tf.keras.losses.binary_crossentropy(self.y, pred, axis=0)\n",
    "\n",
    "            grad = tape.gradient(loss, {\n",
    "                \"W\" : self.W,\n",
    "                \"b\" : self.b\n",
    "            })\n",
    "            self.W = self.W - self.learning_rate * grad['W']\n",
    "            self.b = self.b - self.learning_rate * grad['b']\n",
    "        print(loss.numpy()[0])\n",
    "    \n",
    "    def convert_params(self):\n",
    "        W_temp = self.W.numpy().tolist()\n",
    "        W = [w[0] for w in W_temp]\n",
    "        b = self.b.numpy().tolist()\n",
    "        return W, b\n",
    "    \n",
    "    def send_local_model(self):\n",
    "        param_id = \"local_\" + str(self.round) + \"_\" + self.name\n",
    "        W, b = self.convert_params()\n",
    "        req.post_param(param_id, W, b)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data2d.npz')\n",
    "X, y = data['X'], data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 15:54:38.366441: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-23 15:54:38.366761: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "client_2 = Client(\"2\", X[len(X)//2:], y[len(X)//2:].reshape((35, 1)), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2530861709327289\n",
      "Trained global model.\n"
     ]
    }
   ],
   "source": [
    "client_2.check_trian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17779173452958144\n",
      "Trained global model.\n",
      "0.14189486617276076\n",
      "Trained global model.\n",
      "0.12149112207146102\n",
      "Trained global model.\n",
      "0.10795511109493124\n",
      "Trained global model.\n",
      "0.09815960567391627\n",
      "Trained global model.\n",
      "0.09065106683220536\n",
      "Trained global model.\n",
      "0.08465614034043084\n",
      "Trained global model.\n",
      "0.07972238044501621\n",
      "Trained global model.\n",
      "0.0755658944935677\n",
      "Trained global model.\n",
      "0.07199868319396296\n",
      "Trained global model.\n",
      "0.06889079945221205\n",
      "Trained global model.\n",
      "0.06614923330208912\n",
      "Trained global model.\n",
      "0.06370545586558404\n",
      "Trained global model.\n",
      "0.06150772642959458\n",
      "Trained global model.\n",
      "0.05951615468703701\n",
      "Trained global model.\n",
      "0.05769942567917381\n",
      "Trained global model.\n",
      "0.05603256519909634\n",
      "Trained global model.\n",
      "0.054495377032360785\n",
      "Trained global model.\n",
      "0.053071326074900387\n",
      "Trained global model.\n",
      "0.051746724612092086\n",
      "Trained global model.\n",
      "0.05051012920324955\n",
      "Trained global model.\n",
      "0.049351886715175604\n",
      "Trained global model.\n",
      "0.04826378782691107\n",
      "Trained global model.\n",
      "0.04723879919733025\n",
      "Trained global model.\n",
      "0.046270854035911795\n",
      "Trained global model.\n",
      "0.045354686603018\n",
      "Trained global model.\n",
      "0.04448570014971013\n",
      "Trained global model.\n",
      "0.04365986059322818\n",
      "Trained global model.\n",
      "0.04287361020106854\n",
      "Trained global model.\n",
      "0.04212379697794382\n",
      "Trained global model.\n",
      "0.041407616484531014\n",
      "Trained global model.\n",
      "0.04072256357868068\n",
      "Trained global model.\n",
      "0.040066392136620936\n",
      "Trained global model.\n",
      "0.03943708123771993\n",
      "Trained global model.\n",
      "0.038832806619544406\n",
      "Trained global model.\n",
      "0.0382519164572396\n",
      "Trained global model.\n",
      "0.03769291071203321\n",
      "Trained global model.\n",
      "0.03715442344197932\n",
      "Trained global model.\n",
      "0.03663520758420311\n",
      "Trained global model.\n",
      "0.03613412180948011\n",
      "Trained global model.\n",
      "0.035650119122657434\n",
      "Trained global model.\n",
      "0.035182236940452236\n",
      "Trained global model.\n",
      "0.03472958842476664\n",
      "Trained global model.\n",
      "0.03429135488729663\n",
      "Trained global model.\n",
      "0.03386677911176849\n",
      "Trained global model.\n",
      "0.03345515946506911\n",
      "Trained global model.\n",
      "0.033055844688977966\n",
      "Trained global model.\n",
      "0.03266822928104159\n",
      "Trained global model.\n",
      "0.032291749387061745\n",
      "Trained global model.\n",
      "0.03192587913923142\n",
      "Trained global model.\n",
      "0.03157012738360535\n",
      "Trained global model.\n",
      "0.031224034748664187\n",
      "Trained global model.\n",
      "0.03088717101352177\n",
      "Trained global model.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n",
      "Round is stil going on.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    time.sleep(5)\n",
    "    client_2.check_trian()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
